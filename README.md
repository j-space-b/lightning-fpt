# Lightning FPT (Finetuning Pre-Trained Transformers)

Lightning FPT is focused on [Lightning AI's](https://lightning.ai) implemenations of LLaMA, Falcon, Pythia, and INCITE, based on nanoGPT:

- [Lit-Parrot](https://github.com/Lightning-AI/lit-parrot)
- [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)

## Toolkit

### Frameworks

- [PyTorch](https://pytorch.org/docs/stable/index.html)
- [Lightning](https://github.com/Lightning-AI/lightning)
- [SheepRL](https://github.com/Eclectic-Sheep/sheeprl)

### Pretrained weights

- [Falcon](https://huggingface.co/tiiuae/falcon-40b)
- [OpenLLaMA](https://github.com/openlm-research/open_llama)

### Instruction Tuning Dataset

- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)

### Parameter Efficient Finetuning

- [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter)
- [LoRA](https://github.com/microsoft/LoRA)

### Evaluation Harnesses:

- Lightning AI's [lm-evaluation-harness](https://github.com/Lightning-AI/lm-evaluation-harness)
- Eleuther AI's [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

## Tutorials and References

- [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)
- [Finetuning Falcon LLMs More Efficiently With LoRA and Adapters](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)
- [Understanding Parameter-Efficient Finetuning](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
- [Parameter-Efficient LLM Finetuning With LoRA](https://lightning.ai/pages/community/tutorial/lora-llm/)
- [Accelerating Large Language Models with Mixed-Precision Techniques](https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/)
